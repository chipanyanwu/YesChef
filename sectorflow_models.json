[
    {
        "id": "5284cbf5-873b-40cb-abc4-c62a792c2113",
        "name": "Amazon Nova Micro",
        "icon": "AMAZON",
        "baseModel": "Amazon_Nova-Micro",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T13:06:41.459847",
        "updated": "2025-01-27T13:06:41.459847",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 1,
        "description": "A text-only model delivering low-latency responses at a very low cost, suitable for tasks like text summarization, translation, and content classification."
    },
    {
        "id": "8b415799-682e-49b9-97ab-1f6e99162177",
        "name": "Amazon Nova Lite",
        "icon": "AMAZON",
        "baseModel": "Amazon_Nova-Lite",
        "contextWindow": 300000,
        "chatCredits": 3,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T13:05:44.055141",
        "updated": "2025-01-27T13:06:50.028251",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "A cost-effective multimodal model that processes text, image, and video inputs with high speed, ideal for real-time applications requiring quick responses."
    },
    {
        "id": "aa59c1a9-e2b2-4c53-bfa6-cf5374deec42",
        "name": "Amazon Nova Pro",
        "icon": "AMAZON",
        "baseModel": "Amazon_Nova-Pro",
        "contextWindow": 300000,
        "chatCredits": 5,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T13:04:34.181493",
        "updated": "2025-01-27T13:05:53.708485",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "A highly capable multimodal model offering an optimal balance of accuracy, speed, and cost for a wide range of tasks, including text, image, and video processing."
    },
    {
        "id": "f2321b16-76e2-42a1-8817-2ad9573a8708",
        "name": "Claude 3 Sonnet",
        "icon": "ANTHROPIC",
        "baseModel": "anthropic.claude-3-sonnet-20240229-v1:0",
        "contextWindow": 200000,
        "chatCredits": 12,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-01-20T10:19:52.968",
        "updated": "2024-01-20T10:19:52.968",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments. Claude 3 Sonnet also has vision capabilities."
    },
    {
        "id": "95d87e9c-4e62-4473-81d6-eaab7f5d9149",
        "name": "Claude 3.5 Sonnet",
        "icon": "ANTHROPIC",
        "baseModel": "anthropic.claude-3-5-sonnet-v2@20241022",
        "contextWindow": 200000,
        "chatCredits": 12,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:04:31.243501",
        "updated": "2025-02-15T23:04:31.243501",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Claude 3.5 Sonnet, demonstrates exceptional capabilities across a diverse range of tasks and evaluations while also outperforming Claude 3 Opus. Claude 3.5 Sonnet also has vision capabilities."
    },
    {
        "id": "c29cb096-5c1e-4a0f-892e-d47b2dfa8279",
        "name": "Claude 3.5 Haiku",
        "icon": "ANTHROPIC",
        "baseModel": "anthropic.claude-3-5-haiku-20241022-v1:0",
        "contextWindow": 200000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-01-20T10:19:52.968",
        "updated": "2024-01-20T10:19:52.968",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "The fastest model in Anthropic’s lineup, Claude 3.5 Haiku is versatile, excelling in coding, reasoning, data extraction, and moderation tasks. Perfect for low-latency applications like chatbots and real-time use cases."
    },
    {
        "id": "d165966d-95f7-4328-95aa-e32fb6e46a5c",
        "name": "Claude 3 Opus ",
        "icon": "ANTHROPIC",
        "baseModel": "anthropic.claude-3-opus-20240229-v1:0",
        "contextWindow": 200000,
        "chatCredits": 30,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-19T13:03:24.035552",
        "updated": "2024-04-19T13:03:24.035552",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Opus is a highly intelligent model with reliable performance on complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus also has vision capabilities."
    },
    {
        "id": "01442edd-1154-4d9c-b4e6-151ba35f8f56",
        "name": "OpenAi GPT-4o",
        "icon": "CHATGPT",
        "baseModel": "gpt-4o",
        "contextWindow": 128000,
        "chatCredits": 14,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:04:31.243501",
        "updated": "2025-02-15T23:04:31.243501",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "frequencyPenalty",
                "name": "Frequency Penalty",
                "description": "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "n",
                "name": "N",
                "description": "How many chat completion choices to generate for each input message. Keep n as 1 to minimize costs."
            },
            {
                "fieldName": "presencePenalty",
                "name": "Presence Penalty",
                "description": "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
            },
            {
                "fieldName": "systemInstructions",
                "name": "System Instructions",
                "description": "How should the model Respond"
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "GPT-4o is OpenAI's most advanced model to date. This multimodal model handles both text and image inputs while generating text outputs. Matching the intelligence of GPT-4 Turbo, it is remarkably more efficient, delivering text at twice the speed and at half the cost. Additionally, GPT-4o exhibits the highest vision performance and excels in non-English languages compared to previous OpenAI models."
    },
    {
        "id": "ef8250b8-03cc-4fee-9a6b-226a52eec046",
        "name": "OpenAi Dall-e-3",
        "icon": "CHATGPT",
        "baseModel": "dall-e-3",
        "contextWindow": 4000,
        "chatCredits": 180,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:04:31.243501",
        "updated": "2025-02-15T23:04:31.243501",
        "usesSystemPrompts": true,
        "imageGen": true,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 10,
        "description": "DALL·E 3 is the OpenAI's highest quality model for image generation."
    },
    {
        "id": "c7c5432b-a9f1-4921-8ddc-71b7713c9bdc",
        "name": "OpenAi GPT-4o mini",
        "icon": "CHATGPT",
        "baseModel": "gpt-4o-mini",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:05:16.27253",
        "updated": "2025-02-15T23:05:16.272539",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "GPT-4o-mini is a small, yet powerful model that is significantly more affordable than previous models. GPT-4o mini scores quite high on benchmarks and is preferred over GPT-4 by user votes on the LMSYS Chatbot Arena leaderboard. It supports text and vision, with future plans for image, video, and audio inputs and outputs. With a context window of 128K tokens and up to 16K output tokens per request, it excels in reasoning, math, coding, and multimodal tasks."
    },
    {
        "id": "3ed5d57e-6c4a-4389-b00f-a73f78267549",
        "name": "OpenAi  ChatGPT 3.5",
        "icon": "CHATGPT",
        "baseModel": "gpt-3.5-turbo-0125",
        "contextWindow": 16385,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2023-11-23T14:31:07.326",
        "updated": "2023-11-23T14:31:07.326",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "frequencyPenalty",
                "name": "Frequency Penalty",
                "description": "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "n",
                "name": "N",
                "description": "How many chat completion choices to generate for each input message. Keep n as 1 to minimize costs."
            },
            {
                "fieldName": "presencePenalty",
                "name": "Presence Penalty",
                "description": "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
            },
            {
                "fieldName": "systemInstructions",
                "name": "System Instructions",
                "description": "How should the model Respond"
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "GPT-3.5 Turbo is a fast, inexpensive model for simple tasks. We recommend using GPT-4o mini where GPT-3.5 turbo was used previously. "
    },
    {
        "id": "6cb4062d-50c5-443d-b549-2996ced4b42a",
        "name": "OpenAi GPT-o1",
        "icon": "CHATGPT",
        "baseModel": "o1-preview",
        "contextWindow": 128000,
        "chatCredits": 50,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "The preview version of OpenAI's o1 family mirrors o1-mini’s STEM strengths. Ideal for those exploring early access experimental features."
    },
    {
        "id": "6db9dce4-a112-4e87-a5da-e092afb2fd7e",
        "name": "OpenAi GPT 4 Turbo",
        "icon": "CHATGPT",
        "baseModel": "gpt-4-turbo",
        "contextWindow": 128000,
        "chatCredits": 30,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-01-20T08:32:56.094",
        "updated": "2024-01-20T08:32:56.094",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "systemInstructions",
                "name": "System Instructions",
                "description": "How should the model Respond"
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "GPT-4 Turbo has a knowledge cutoff of April 2023 and introduces a 128k context window (the equivalent of 300 pages of text in a single prompt). Prior to GPT-4, this was OpenAi's top rated model. "
    },
    {
        "id": "33f9a092-36e9-4a09-9269-ce9191aa2e14",
        "name": "OpenAi GPT-o1 mini",
        "icon": "CHATGPT",
        "baseModel": "o1-mini",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\r\n\r\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology."
    },
    {
        "id": "aa808bd6-926f-42b3-b13e-1a6b8bfa2e06",
        "name": "Cohere Command R",
        "icon": "COHERE",
        "baseModel": "Cohere_Command-R-08-2024",
        "contextWindow": 128000,
        "chatCredits": 6,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-19T11:11:44.80847",
        "updated": "2025-01-27T13:09:10.356432",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "A robust language model from Cohere's Command series, released in August 2024, tailored for a variety of natural language processing applications."
    },
    {
        "id": "d4c52732-787e-4dec-b974-b1806035fb44",
        "name": "Cohere Command R+",
        "icon": "COHERE",
        "baseModel": "Cohere_Command-R-Plus-08-2024",
        "contextWindow": 128000,
        "chatCredits": 12,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:04:31.243501",
        "updated": "2025-02-15T23:04:31.243501",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "An enhanced language model from Cohere's Command series, released in August 2024, designed for advanced natural language understanding and generation tasks."
    },
    {
        "id": "c5faaf44-bfae-4f19-aa89-bfcc327e5837",
        "name": "DeepSeek R1",
        "icon": "DEEPSEEK",
        "baseModel": "DeepSeek_DeepSeek-R1",
        "contextWindow": 64000,
        "chatCredits": 2,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:13:20.781303",
        "updated": "2025-02-15T23:13:20.78131",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "An open-source model delivering performance on par with OpenAI o1, featuring fully open reasoning tokens."
    },
    {
        "id": "17b0afb4-8798-495c-9f47-2bafd4ba22a5",
        "name": "DeepSeek R1 Distill Llama 70B",
        "icon": "DEEPSEEK",
        "baseModel": "DeepSeek_DeepSeek-R1-Distill-Llama-70B",
        "contextWindow": 128000,
        "chatCredits": 2,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T13:03:17.762933",
        "updated": "2025-01-27T13:03:17.762933",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "A distilled large language model based on Llama-3.3-70B-Instruct, achieving high performance across multiple benchmarks."
    },
    {
        "id": "897f0433-5dfc-475f-a257-4ae2611b3911",
        "name": "Deepseek V3 Chat",
        "icon": "DEEPSEEK",
        "baseModel": "Deepseek_Deepseek-V3-chat",
        "contextWindow": 64000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-18T19:03:33.126",
        "updated": "2025-01-27T12:13:39.167923",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "DeepSeek V3 builds on its predecessors in coding and instruction-following capabilities, trained on 15 trillion tokens for unparalleled open-source performance. It rivals closed-source models across most benchmarks"
    },
    {
        "id": "a9a09432-9b41-47e5-8c49-32713da7280a",
        "name": "Google Gemini Pro 128k",
        "icon": "GEMINI",
        "baseModel": "gemini-1.5-pro-128k",
        "contextWindow": 128000,
        "chatCredits": 5,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-02-15T23:04:31.243501",
        "updated": "2025-02-15T23:04:31.243501",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Gemini 1.5 Pro is a versatile foundation model that excels in multimodal tasks like visual understanding, classification, summarization, and content creation from text, images, audio, and video. It processes various visual and text inputs such as photographs, documents, infographics, and screenshots. Supports 128k token context size."
    },
    {
        "id": "4a2aaee9-a323-418f-a6c7-cadb7e63bf3d",
        "name": "Google Imagen 2",
        "icon": "GEMINI",
        "baseModel": "google-imagen-2",
        "contextWindow": 4000,
        "chatCredits": 60,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-06-23T21:44:35.014285",
        "updated": "2025-02-05T21:29:27.771218",
        "usesSystemPrompts": true,
        "imageGen": true,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 10,
        "description": "Google DeepMind's Imagen 2 is an advanced text-to-image AI model that produces high-quality, detailed images with improved accuracy in rendering human features and reducing visual artifacts. Trained on superior data, it offers enhanced semantic alignment with text prompts and greater photorealism across various styles and applications."
    },
    {
        "id": "4c30000b-2028-4089-bd55-a25caadbe000",
        "name": "Google Gemini Flash 1M",
        "icon": "GEMINI",
        "baseModel": "gemini-1.5-flash",
        "contextWindow": 1000000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-05-23T07:39:04.220617",
        "updated": "2025-02-05T18:19:00.016194",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Gemini 1.5 Flash is a versatile foundation model excelled at multimodal tasks like visual understanding, classification, and content creation across text, image, audio, and video. Designed for high-volume, high-frequency tasks, it offers comparable quality to other Gemini Pro models but at a significantly reduced cost, making it ideal for applications like chat assistants and on-demand content generation where speed, scale, and cost efficiency are crucial. Supports 1 million token context size"
    },
    {
        "id": "c4b14a75-6aed-4358-ac87-07fb0e72caa4",
        "name": "Google Gemini Pro 2M",
        "icon": "GEMINI",
        "baseModel": "gemini-1.5-pro",
        "contextWindow": 2000000,
        "chatCredits": 9,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-05-23T07:39:07.884881",
        "updated": "2024-05-23T07:39:07.884881",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Gemini 1.5 Pro is a versatile foundation model that excels in multimodal tasks like visual understanding, classification, summarization, and content creation from text, images, audio, and video. It processes various visual and text inputs such as photographs, documents, infographics, and screenshots. Supports 2 million token context size."
    },
    {
        "id": "0952665e-1385-4ed0-9166-cb873c01a7c9",
        "name": "Google Gemini Flash 128k",
        "icon": "GEMINI",
        "baseModel": "gemini-1.5-flash-128k",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-05-16T14:34:07.89646",
        "updated": "2025-02-05T18:18:22.229919",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Gemini 1.5 Flash is a versatile foundation model excelled at multimodal tasks like visual understanding, classification, and content creation across text, image, audio, and video. Designed for high-volume, high-frequency tasks, it offers comparable quality to other Gemini Pro models but at a significantly reduced cost, making it ideal for applications like chat assistants and on-demand content generation where speed, scale, and cost efficiency are crucial. Supports 128k token context size."
    },
    {
        "id": "fc5da09c-4981-4de5-a62c-b1ef16498bdb",
        "name": "Google Imagen3",
        "icon": "GEMINI",
        "baseModel": "google-imagen-3.0-generate-001",
        "contextWindow": 4000,
        "chatCredits": 105,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": true,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Google's Imagen 3 is a state-of-the-art text-to-image generation model that offers significant improvements in prompt understanding and image quality. It excels in generating a wide range of visual styles, from photorealistic landscapes to richly textured oil paintings and whimsical claymation scenes. Imagen 3 stands out for its ability to capture fine details from longer prompts and understand natural, everyday language without the need for complex prompt engineering. The model generates visually rich, high-quality images with good lighting and composition, accurately rendering small details like fine wrinkles on a person's hand and complex textures. This versatility and precision make Imagen 3 an excellent choice for businesses seeking top-tier, diverse visual content generation for applications in marketing, advertising, and creative design."
    },
    {
        "id": "627d33e5-1112-475c-b766-e8b3a001daf7",
        "name": "Google Imagen3 Fast",
        "icon": "GEMINI",
        "baseModel": "google-imagen-3.0-fast-generate-001",
        "contextWindow": 4000,
        "chatCredits": 50,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": true,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Google's Imagen 3 Fast is an optimized version of the Imagen 3 model, designed for quicker image generation while maintaining high-quality output. It shares the core capabilities of its counterpart, including enhanced prompt understanding and versatility in style generation. Imagen 3 Fast can produce a wide range of visual styles, from photorealistic images to artistic renderings, and understands prompts written in natural language. While optimized for speed, it still delivers visually rich images with good lighting, composition, and attention to detail. This faster version is ideal for businesses that require rapid content creation without significantly compromising on image quality, making it suitable for time-sensitive projects in marketing, content creation, and other fields where quick turnaround is essential."
    },
    {
        "id": "e970f369-b331-4cad-87c1-9f7c5990203d",
        "name": "XAi Grok 2 Vision",
        "icon": "GROK",
        "baseModel": "X_AI-Grok2-Vision",
        "contextWindow": 128000,
        "chatCredits": 3,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-18T19:03:33.126",
        "updated": "2024-07-18T19:03:33.126",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Advanced image-based AI with refined instruction-following, multilingual capability, and enhanced steerability for intuitive, visually aware applications (object detection, image analysis, etc.)."
    },
    {
        "id": "d279d5db-9812-421c-8a9c-beb941e0ee1a",
        "name": "XAi Grok 2",
        "icon": "GROK",
        "baseModel": "X_AI-Grok2",
        "contextWindow": 128000,
        "chatCredits": 3,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-18T19:03:33.126",
        "updated": "2024-07-18T19:03:33.126",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "A highly intelligent and steerable LLM offering improved accuracy, multilingual support, and flexibility. Tailored for developers creating cutting-edge applications."
    },
    {
        "id": "b5554c48-b695-45b4-ab01-85a8f520f2bc",
        "name": "Meta Llama-3.2-11B-Vision",
        "icon": "LLAMA",
        "baseModel": "Meta_Llama-3.2-11B-Vision-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Llama 3.2-Vision is a powerful collection of multimodal large language models (LLMs) designed for advanced image reasoning and generation tasks. Available in 11B and 90B parameter sizes, these models excel at visual recognition, image reasoning, captioning, and answering general questions about images. Llama 3.2-Vision outperforms many open-source and closed multimodal models on common industry benchmarks, making it a top choice for businesses seeking cutting-edge visual AI capabilities. Built on the foundation of the Llama 3.1 text-only model and enhanced with a specialized vision adapter, this model offers seamless integration of text and image inputs for a wide range of applications."
    },
    {
        "id": "b06a667e-8150-4a98-9df9-18682a332c65",
        "name": "Meta Llama 3.1 8B",
        "icon": "LLAMA",
        "baseModel": "Llama-3.1-8B-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-19T12:56:02.884",
        "updated": "2024-04-19T12:56:02.884",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "LLAMA2_CHAT",
            "value": "<INST>"
        },
        "modelRating": 3,
        "description": "Llama 3.1 8B: This is the smallest model in the collection, featuring 8 billion parameters. Like the other Llama 3.1 models, it supports multilingual text input and output, as well as code. This model is designed for more general applications but with a smaller computational footprint, making it ideal for less resource-intensive deployments."
    },
    {
        "id": "a8f8e6db-2f17-48fc-89c0-b36f66148350",
        "name": "Meta Llama-3.2-90B-Vision",
        "icon": "LLAMA",
        "baseModel": "Meta_Llama-3.2-90B-Vision-Instruct",
        "contextWindow": 128000,
        "chatCredits": 3,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "Llama 3.2-Vision is a powerful collection of multimodal large language models (LLMs) designed for advanced image reasoning and generation tasks. Available in 11B and 90B parameter sizes, these models excel at visual recognition, image reasoning, captioning, and answering general questions about images. Llama 3.2-Vision outperforms many open-source and closed multimodal models on common industry benchmarks, making it a top choice for businesses seeking cutting-edge visual AI capabilities. Built on the foundation of the Llama 3.1 text-only model and enhanced with a specialized vision adapter, this model offers seamless integration of text and image inputs for a wide range of applications."
    },
    {
        "id": "9e9ccfcf-b238-43c5-afcb-c5b2b4acae2d",
        "name": "Meta Llama 3.3 70B",
        "icon": "LLAMA",
        "baseModel": "Meta_Llama-3.3-70B-Instruct",
        "contextWindow": 128000,
        "chatCredits": 5,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-19T12:57:46.06",
        "updated": "2024-04-19T12:57:46.06",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "LLAMA2_CHAT",
            "value": "<INST>"
        },
        "modelRating": 4,
        "description": "A state-of-the-art multilingual dialogue model with 70B parameters. Outperforms many models in both open-source and closed categories. Supports 8 major languages (English, Spanish, German, etc.)."
    },
    {
        "id": "06307ed1-9e20-4a65-ae08-e91f7e253b78",
        "name": "Meta Llama 3.1 405B",
        "icon": "LLAMA",
        "baseModel": "Llama-3.1-405B-Instruct",
        "contextWindow": 128000,
        "chatCredits": 15,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-24T08:54:21.549176",
        "updated": "2024-07-24T08:54:21.549176",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Llama 3.1 405B: The largest and most powerful model, with 405 billion parameters, is designed for highly demanding applications that require top-tier performance and accuracy. It is the most capable model within the Llama 3.1 collection, but it also requires significant computational resources."
    },
    {
        "id": "1f3533b1-8995-460e-bbe0-67817bddf532",
        "name": "Microsoft Phi 3 Mini",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft-Phi-3-Mini-3.8B-128K-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-30T10:18:28.01767",
        "updated": "2024-04-30T10:18:28.01767",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "Phi-3-mini: This is a 3.8 billion parameter language model. It is designed for simpler tasks and is more accessible for organizations with limited resources. It can be used for content authoring, summarization, and sentiment analysis."
    },
    {
        "id": "33f5be56-e091-4485-af9a-4724de30ac24",
        "name": "Microsoft Phi-3.5-MoE",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft-Phi-3.5-MoE-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Phi-3.5-MoE is a cutting-edge, lightweight open model that builds on the high-quality datasets used for Phi-3, focusing on reasoning-dense data. This multilingual model boasts an impressive 128K token context length, making it versatile for a wide range of applications. Phi-3.5-MoE has undergone extensive enhancement processes to ensure precise instruction adherence and robust safety measures, making it ideal for both commercial and research use. The model's architecture and training approach result in state-of-the-art performance while maintaining efficiency, positioning it as a powerful option for businesses looking to integrate advanced language AI capabilities."
    },
    {
        "id": "294849e9-27e8-4020-95ce-123dea80dcdb",
        "name": "Microsoft Phi 3 Small",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft-Phi-3-Small-128K-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-18T18:35:15.317356",
        "updated": "2024-07-18T18:35:15.317356",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "Phi-3-small: Phi-3-small is a 7 billion parameter language model. It outperforms larger models in various language, reasoning, coding, and math benchmarks. It is well-suited for applications that need strong reasoning and limited compute power."
    },
    {
        "id": "695e49c4-b5ff-498e-b3f5-73f5adee3943",
        "name": "Microsoft Phi 4",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft_Phi-4",
        "contextWindow": 16000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T13:12:00.192925",
        "updated": "2025-01-27T13:12:00.193925",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 1,
        "description": "Microsoft's fourth-generation Phi model, engineered to deliver high performance in complex language understanding and generation tasks."
    },
    {
        "id": "203f2e2e-1c33-4800-a5f1-381caafb8ae4",
        "name": "Microsoft Phi-3.5-Vision",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft_Phi-3.5-Vision-Instruct",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-09-12T18:35:12.852",
        "updated": "2024-09-12T18:35:12.852",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "Phi-3.5-Vision is a cutting-edge, lightweight open multimodal model that builds on high-quality datasets, focusing on reasoning-dense data for both text and vision tasks. As part of the Phi-3 model family, it offers an impressive 128K token context length, making it highly versatile for a wide range of applications. Phi-3.5-Vision has undergone extensive enhancement processes, including supervised fine-tuning and direct preference optimization, to ensure precise instruction adherence and robust safety measures. This state-of-the-art model is fully prepared for both commercial and research applications, offering businesses a powerful and efficient solution for integrating advanced multimodal AI capabilities into their products and services."
    },
    {
        "id": "128a1740-f8ce-49d9-958f-5333af3cf5b2",
        "name": "Microsoft Phi 3 Medium",
        "icon": "MICROSOFT",
        "baseModel": "Microsoft-Phi-3-Medium-14B-128K-Instruct",
        "contextWindow": 128000,
        "chatCredits": 2,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-05-23T07:39:10.523374",
        "updated": "2024-05-23T07:39:10.523374",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Phi-3-medium: With 14 billion parameters, Phi-3-medium is a more powerful language model. It continues the trend of outperforming larger models and is suitable for generative AI applications that require strong reasoning and low latency."
    },
    {
        "id": "ad49f87d-c7e0-4807-8ca1-b00c96eef791",
        "name": "Mistral Large 2 123B",
        "icon": "MISTRAL",
        "baseModel": "MistralAI-Mistral-Large-2-2407-123B",
        "contextWindow": 128000,
        "chatCredits": 9,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-31T19:56:17.321519",
        "updated": "2024-07-31T19:56:17.321519",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 5,
        "description": "Mistral Large 2 is a 123B parameter language model with a 128k context window. It excels in code, math, reasoning, and multilingual tasks, competing with top models like GPT-4. Designed for efficient inference, it offers strong performance across various benchmarks and supports dozens of languages and 80+ coding languages."
    },
    {
        "id": "22f9b9bf-4307-4404-90f4-1df7bc606db8",
        "name": "Mistral 7B v0.3",
        "icon": "MISTRAL",
        "baseModel": "MistralAI_Mistral_7B-v0.3",
        "contextWindow": 32000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-01-20T10:19:52.968",
        "updated": "2025-01-27T13:13:11.596727",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "systemInstructions",
                "name": "System Instructions",
                "description": "How should the model Respond"
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length."
    },
    {
        "id": "063f3352-4dad-4b41-92b8-6590eaf6f976",
        "name": "Mistral-NeMo 12B",
        "icon": "MISTRAL",
        "baseModel": "NV-MistralAI-Mistral-Nemo-12B",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-19T08:18:56.412685",
        "updated": "2024-07-19T08:18:56.412685",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "Mistral-NeMo is a new 12B parameter language model trained jointly by NVIDIA and Mistral AI that excels in common sense reasoning, coding, math, multilingual, and multi-turn chat tasks. It outperforms all models of similar or smaller size on popular benchmarks and in terms of competency at large context lengths."
    },
    {
        "id": "ea7ab6d7-ad6b-48b6-8604-b07239523ce4",
        "name": "MistralAI Pixtral Large",
        "icon": "MISTRAL",
        "baseModel": "MistralAI_Pixtral-Large-2411",
        "contextWindow": 128000,
        "chatCredits": 7,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T12:51:45.60984",
        "updated": "2025-01-27T12:51:45.60984",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": true,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "A powerful 124B parameter, open-weight, multimodal model capable of understanding documents, charts, and natural images."
    },
    {
        "id": "b7460eec-a6fc-4c91-8916-83bb358a9e27",
        "name": "Mixtral 8x7B",
        "icon": "MISTRAL",
        "baseModel": "mistral.mixtral-8x7b-instruct-v0:1",
        "contextWindow": 32000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2023-11-23T14:10:29.398",
        "updated": "2023-11-23T14:10:29.398",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "Mistral's Mixtral 8x7B is a high-quality, open-source sparse mixture of experts (SMoE) model. It's a decoder-only model with 46.7B total parameters, but only uses 12.9B parameters per token, making it efficient and cost-effective. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference and matches or outperforms GPT3.5 on most standard benchmarks. "
    },
    {
        "id": "f5a6eeee-d397-485e-b882-2e3a807f4a8f",
        "name": "Mistral Large",
        "icon": "MISTRAL",
        "baseModel": "MistralAI_Mistral-Large-2411",
        "contextWindow": 128000,
        "chatCredits": 5,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-01-20T10:19:52.968",
        "updated": "2025-01-27T12:36:00.975359",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [
            {
                "fieldName": "addedContext",
                "name": "Added Context",
                "description": "What should the model know about you to provide better answers."
            },
            {
                "fieldName": "maxTokens",
                "name": "Max Tokens",
                "description": "The maximum number of tokens that can be generated in the chat completion."
            },
            {
                "fieldName": "systemInstructions",
                "name": "System Instructions",
                "description": "How should the model Respond"
            },
            {
                "fieldName": "temperature",
                "name": "Temperature",
                "description": "What sampling temperature to use, between 0 and 1 or 2 depending on the model. Higher values will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
            },
            {
                "fieldName": "topP",
                "name": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. "
            }
        ],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Mistral Large is a text generation model that achieves top-tier reasoning capabilities, multilingual understanding, and strong performance on various benchmarks."
    },
    {
        "id": "cceeb215-e4b0-4135-9e3e-f1cc3b401712",
        "name": "Mistral Ministral 3B",
        "icon": "MISTRAL",
        "baseModel": "Ministral-3B",
        "contextWindow": 128000,
        "chatCredits": 1,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T12:56:59.108097",
        "updated": "2025-01-27T13:00:47.725605",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "A compact yet powerful language model designed for efficient performance across various natural language processing tasks."
    },
    {
        "id": "5c1e3ebf-8c17-41c5-bae3-4ca7a5fb8db1",
        "name": "Mixtral 8x22B Instruct",
        "icon": "MISTRAL",
        "baseModel": "Mixtral-8x22B-Instruct-v0.1",
        "contextWindow": 65000,
        "chatCredits": 5,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-04-30T10:18:28.01767",
        "updated": "2024-04-30T10:18:28.01767",
        "usesSystemPrompts": false,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Mixtral 8x22B is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unmatched cost efficiency for its size. It is fluent in 5 languages, has strong mathematics and coding capabilities, and is excellent for application development and tech stack modernisation."
    },
    {
        "id": "bd8fef32-5701-4d11-9d71-b17357412201",
        "name": "Nvidia Nemotron 4 340B",
        "icon": "NVIDIA",
        "baseModel": "Nvidia-Nemotron-4-340B-Instruct",
        "contextWindow": 4000,
        "chatCredits": 2,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-06-28T16:20:28.875964",
        "updated": "2024-06-28T16:20:28.875964",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 3,
        "description": "Nemotron-4-340B-Instruct is a large language model (LLM) that can be used as part of a synthetic data generation pipeline to create training data that helps researchers and developers build their own LLMs. It is a fine-tuned version of the Nemotron-4-340B-Base model, optimized for English-based single and multi-turn chat use-cases. It supports a context length of 4,096 tokens."
    },
    {
        "id": "44cd8380-23ef-496a-ae49-62a2a60b6736",
        "name": "Perplexity 405B Online",
        "icon": "PERPLEXITY",
        "baseModel": "Perplexity_Llama-3.1-Sonar-Huge-128K-405B-Online",
        "contextWindow": 128000,
        "chatCredits": 55,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T12:29:28.927284",
        "updated": "2025-01-27T12:29:28.927284",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "Perplexity's latest model, surpassing earlier versions in cost-efficiency, speed, and performance, with internet access."
    },
    {
        "id": "86883c8c-7435-49d7-87c4-346f83af3fc0",
        "name": "Perplexity 8B Online",
        "icon": "PERPLEXITY",
        "baseModel": "Perplexity_Llama-3.1-Sonar-Small-128K-8B-Online",
        "contextWindow": 128000,
        "chatCredits": 50,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T12:32:25.764474",
        "updated": "2025-01-27T12:32:25.764474",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 2,
        "description": "A cost-effective online model providing accurate and timely information."
    },
    {
        "id": "d0dd6c37-952e-4c66-a8e4-60bae6f23335",
        "name": "Perplexity 70B Online",
        "icon": "PERPLEXITY",
        "baseModel": "Perplexity_Llama-3.1-Sonar-Large-128K-70B-Online",
        "contextWindow": 128000,
        "chatCredits": 51,
        "active": true,
        "internalUseOnly": false,
        "created": "2025-01-27T12:31:18.748426",
        "updated": "2025-01-27T12:31:18.748426",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": false,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": ""
    },
    {
        "id": "0816b44c-3cd3-4bc5-8c78-b9d16c8211b5",
        "name": "Stable Diffusion XL 1.0",
        "icon": "STABLEDIFF",
        "baseModel": "stabilityai-stable-diffusion-xl-v1",
        "contextWindow": 4000,
        "chatCredits": 180,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-06-23T21:44:35.014285",
        "updated": "2024-06-23T21:44:35.014285",
        "usesSystemPrompts": true,
        "imageGen": true,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 10,
        "description": "Image model from Stability AI and requires only a few words to create complex, detailed, and aesthetically pleasing images. Distinct images can be prompted without having any particular 'feel' imparted by the model, ensuring absolute freedom of style."
    },
    {
        "id": "6d340bb9-3e34-4599-996f-a55aae75c5c8",
        "name": "01 AI - Yi Large",
        "icon": "YI",
        "baseModel": "01_AI-Yi-Large-34B",
        "contextWindow": 32768,
        "chatCredits": 4,
        "active": true,
        "internalUseOnly": false,
        "created": "2024-07-18T19:30:43.658177",
        "updated": "2024-07-18T19:30:43.658177",
        "usesSystemPrompts": true,
        "imageGen": false,
        "hasVision": false,
        "settingsAvailable": [],
        "instructionFollowing": true,
        "promptWrapper": {
            "wrapperName": "CHAT_ML",
            "value": "chat_ml"
        },
        "modelRating": 4,
        "description": "Yi-Large is a state-of-the-art large language model by 01.AI, excelling in code generation, logic, and mathematical reasoning. It builds on the November 2023 Yi-34B open-source model, showing exceptional performance on benchmarks, particularly in coding, math, and comprehensive reasoning, comparable to GPT-4 and Claude3. Yi-Large also excels in multilingual tasks, ranking highly in Chinese, Spanish, Japanese, German, and French, aligning with 01.AI vision to make AGI accessible and beneficial globally."
    }
]